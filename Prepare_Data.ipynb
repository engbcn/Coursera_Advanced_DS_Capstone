{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing other standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from pylab import text\n",
    "import csv\n",
    "from PIL import Image\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for loading dataset from 'pickle' file\n",
    "def load_rgb_data(file):\n",
    "    # Opening 'pickle' file and getting images\n",
    "    with open(file, 'rb') as f:\n",
    "        d = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "        # At the same time method 'astype()' used for converting ndarray from int to float\n",
    "        # It is needed to divide float by float when applying Normalization\n",
    "        x = d['features'].astype(np.float32)   # 4D numpy.ndarray type, for train = (34799, 32, 32, 3)\n",
    "        y = d['labels']                        # 1D numpy.ndarray type, for train = (34799,)\n",
    "        s = d['sizes']                         # 2D numpy.ndarray type, for train = (34799, 2)\n",
    "        c = d['coords']                        # 2D numpy.ndarray type, for train = (34799, 4)\n",
    "        \"\"\"\n",
    "        Data is a dictionary with four keys:\n",
    "            'features' - is a 4D array with raw pixel data of the traffic sign images,\n",
    "                         (number of examples, width, height, channels).\n",
    "            'labels'   - is a 1D array containing the label id of the traffic sign image,\n",
    "                         file label_names.csv contains id -> name mappings.\n",
    "            'sizes'    - is a 2D array containing arrays (width, height),\n",
    "                         representing the original width and height of the image.\n",
    "            'coords'   - is a 2D array containing arrays (x1, y1, x2, y2),\n",
    "                         representing coordinates of a bounding frame around the image.\n",
    "        \"\"\"\n",
    "\n",
    "    # Returning ready data\n",
    "    return x, y, s, c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for getting texts for every class - labels\n",
    "def label_text(file):\n",
    "    # Defining list for saving label in order from 0 to 42\n",
    "    label_list = []\n",
    "\n",
    "    # Opening 'csv' file and getting image's labels\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        # Going through all rows\n",
    "        for row in reader:\n",
    "            # Adding from every row second column with name of the label\n",
    "            label_list.append(row[1])\n",
    "        # Deleting the first element of list because it is the name of the column\n",
    "        del label_list[0]\n",
    "    # Returning resulted list\n",
    "    return label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for getting random image of one label\n",
    "def random_image(x_train, y_train, y_number):\n",
    "    # Getting indexes of needed 'y_number' from 'y_train'\n",
    "    # Defining True - False array\n",
    "    image_indexes = np.where(y_train == y_number)\n",
    "    # Getting random index of needed label\n",
    "    # 'np.bincount(y_train)' - array with number of examples for every label\n",
    "    # 'np.bincount(y_train)[y_number] - 1' - number of examples for 'y_number' label\n",
    "    random_index = np.random.randint(0, np.bincount(y_train)[y_number] - 1)\n",
    "    # Returning random image from 'x_train'\n",
    "    # 'x_train[image_indexes]' - returns array with only 'y_number' label\n",
    "    # 'x_train[image_indexes][random_index]' - random image of needed label\n",
    "    return x_train[image_indexes][random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for equalization training dataset\n",
    "def equalize_training_dataset(x_train, y_train):\n",
    "    # Getting number of examples for every label\n",
    "    number_of_examples_for_every_label = np.bincount(y_train)\n",
    "    # Calculating total amount of unique labels\n",
    "    number_of_labels = np.arange(len(number_of_examples_for_every_label))\n",
    "\n",
    "    # Iterating over all number of labels\n",
    "    # Showing progress ber with 'tqdm'\n",
    "    for i in tqdm(number_of_labels):\n",
    "        # Calculating how many examples is needed to add for current label\n",
    "        # 'np.mean(number_of_examples_for_every_label)' - average number over examples for every label\n",
    "        number_of_examples_to_add = int(np.mean(number_of_examples_for_every_label) * 2.5) - \\\n",
    "                                    number_of_examples_for_every_label[i]\n",
    "\n",
    "        # Defining temporary arrays for collecting new images\n",
    "        x_temp = []\n",
    "        y_temp = []\n",
    "\n",
    "        # Getting random image from current label\n",
    "        # Transforming it and adding to the temporary arrays\n",
    "        for j in range(number_of_examples_to_add):\n",
    "            getting_random_image = random_image(x_train, y_train, i)\n",
    "            x_temp.append(transformation_brightness_rotation(getting_random_image))\n",
    "            y_temp.append(i)\n",
    "\n",
    "        x_train = np.append(x_train, np.array(x_temp), axis=0)\n",
    "        y_train = np.append(y_train, np.array(y_temp), axis=0)\n",
    "\n",
    "    return x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for transformation: brightness + rotation\n",
    "def transformation_brightness_rotation(image):\n",
    "    return brightness_changing(rotation_changing(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for changing brightness\n",
    "def brightness_changing(image):\n",
    "    # Converting firstly image from RGB to HSV\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    # Defining random value for changing brightness\n",
    "    random_brightness = 0.25 + np.random.uniform()\n",
    "    # Implementing changing of Value channel of HSV image\n",
    "    image_hsv[:, :, 2] = image_hsv[:, :, 2] * random_brightness\n",
    "    # Converting HSV changed image to RGB\n",
    "    image_rgb = cv2.cvtColor(image_hsv, cv2.COLOR_HSV2RGB)\n",
    "    # Returning image with changed brightness\n",
    "    return image_rgb\n",
    "\n",
    "# Defining function for changing rotation of image\n",
    "def rotation_changing(image):\n",
    "    # Defining angle range\n",
    "    angle_range = 25\n",
    "    # Defining angle rotation\n",
    "    angle_rotation = np.random.uniform(angle_range) - angle_range / 2\n",
    "    # Getting shape of image\n",
    "    rows, columns, channels = image.shape\n",
    "    # Implementing rotation\n",
    "    # Calculating Affine Matrix\n",
    "    affine_matrix = cv2.getRotationMatrix2D((columns / 2, rows / 2), angle_rotation, 1)\n",
    "    # Warping original image with Affine Matrix\n",
    "    rotated_image = cv2.warpAffine(image, affine_matrix, (columns, rows))\n",
    "    # Returning rotated image\n",
    "    return rotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for Local Histogram Equalization\n",
    "def local_histogram_equalization(image):\n",
    "    # Creating CLAHE object with arguments\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
    "\n",
    "    # Applying Local Histogram Equalization and returning resulted image\n",
    "    return clahe.apply(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for preprocessing loaded data\n",
    "def preprocess_data(d, shuffle=False, lhe=False, norm_255=False, mean_norm=False, std_norm=False,\n",
    "                    transpose=True, colour='rgb'):\n",
    "    # Applying Shuffling\n",
    "    if shuffle:\n",
    "        # Shuffle data\n",
    "        # Multi-dimensional arrays are only shuffled along the first axis\n",
    "        # By using seed we generate two times the same random numbers\n",
    "        # And save appropriate connection: image --> label\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['x_train'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['y_train'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['x_validation'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['y_validation'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['x_test'])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(d['y_test'])\n",
    "        # Also, possible to do like following:\n",
    "        # x_train, y_train = shuffle(x_train, y_train)\n",
    "        # This function is from sklearn library:\n",
    "        # from sklearn.utils import shuffle\n",
    "\n",
    "    # Applying Local Histogram Equalization\n",
    "    if lhe:\n",
    "        # Function map applies first argument to all elements of the second argument\n",
    "        # First argument in our case is a function\n",
    "        # Second argument in our case is np array\n",
    "        # We need to slice it in order to pass into the function only (32, 32) and not (1, 32, 32)\n",
    "        # Also, map functions applies to first argument all images of the second argument\n",
    "        # In our case it is a number of d['x_train'].shape[0]\n",
    "        # Result we wrap with list and then list convert to np.array\n",
    "        # And reshaping it to make it again 4D tensor\n",
    "\n",
    "        d['x_train'] = list(map(local_histogram_equalization, d['x_train'][:, 0, :, :].astype(np.uint8)))\n",
    "        d['x_train'] = np.array(d['x_train'])\n",
    "        d['x_train'] = d['x_train'].reshape(d['x_train'].shape[0], 1, 32, 32)\n",
    "        d['x_train'] = d['x_train'].astype(np.float32)\n",
    "        d['x_validation'] = list(map(local_histogram_equalization, d['x_validation'][:, 0, :, :].astype(np.uint8)))\n",
    "        d['x_validation'] = np.array(d['x_validation'])\n",
    "        d['x_validation'] = d['x_validation'].reshape(d['x_validation'].shape[0], 1, 32, 32)\n",
    "        d['x_validation'] = d['x_validation'].astype(np.float32)\n",
    "        d['x_test'] = list(map(local_histogram_equalization, d['x_test'][:, 0, :, :].astype(np.uint8)))\n",
    "        d['x_test'] = np.array(d['x_test'])\n",
    "        d['x_test'] = d['x_test'].reshape(d['x_test'].shape[0], 1, 32, 32)\n",
    "        d['x_test'] = d['x_test'].astype(np.float32)\n",
    "\n",
    "    # Applying /255.0 Normalization\n",
    "    if norm_255:\n",
    "        # Normalizing whole data by dividing /255.0\n",
    "        d['x_train'] = d['x_train'].astype(np.float32) / 255.0\n",
    "        d['x_validation'] /= 255.0\n",
    "        d['x_test'] /= 255.0\n",
    "\n",
    "        # Preparing 'mean image'\n",
    "        # Subtracting the dataset by 'mean image' serves to center the data\n",
    "        # It helps for each feature to have a similar range and gradients don't go out of control.\n",
    "        # Calculating 'mean image' from training dataset along the rows by specifying 'axis=0'\n",
    "        # We CALCULATE 'mean image' ONLY FROM TRAINING dataset\n",
    "        # Calculating mean image from training dataset along the rows by specifying 'axis=0'\n",
    "        mean_image = np.mean(d['x_train'], axis=0)  # numpy.ndarray (3, 32, 32)\n",
    "        # Saving calculated 'mean_image' into 'pickle' file\n",
    "        # We will use it when preprocess input data for classifying\n",
    "        # We will need to subtract input image for classifying\n",
    "        # As we're doing now for training, validation and testing data\n",
    "        dictionary = {'mean_image_' + colour: mean_image}\n",
    "        with open('mean_image_' + colour + '.pickle', 'wb') as f_mean_image:\n",
    "            pickle.dump(dictionary, f_mean_image)\n",
    "\n",
    "        # Preparing 'std image'\n",
    "        # Calculating standard deviation from training dataset along the rows by specifying 'axis=0'\n",
    "        std = np.std(d['x_train'], axis=0)  # numpy.ndarray (3, 32, 32)\n",
    "        # Saving calculated 'std' into 'pickle' file\n",
    "        # We will use it when preprocess input data for classifying\n",
    "        # We will need to divide input image for classifying\n",
    "        # As we're doing now for training, validation and testing data\n",
    "        dictionary = {'std_' + colour: std}\n",
    "        with open('std_' + colour + '.pickle', 'wb') as f_std:\n",
    "            pickle.dump(dictionary, f_std)\n",
    "\n",
    "    # Applying Mean Normalization\n",
    "    if mean_norm:\n",
    "        # Normalizing data by subtracting with 'mean image'\n",
    "        # Getting saved data for 'mean image'\n",
    "        # Opening file for reading in binary mode\n",
    "        with open('mean_image_' + colour + '.pickle', 'rb') as f:\n",
    "            mean_image = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "\n",
    "        d['x_train'] -= mean_image['mean_image_' + colour]\n",
    "        d['x_validation'] -= mean_image['mean_image_' + colour]\n",
    "        d['x_test'] -= mean_image['mean_image_' + colour]\n",
    "\n",
    "    # Applying STD Normalization\n",
    "    if std_norm:\n",
    "        # Normalizing data by dividing with 'standard deviation'\n",
    "        # Getting saved data for 'std image'\n",
    "        # Opening file for reading in binary mode\n",
    "        with open('std_' + colour + '.pickle', 'rb') as f:\n",
    "            std = pickle.load(f, encoding='latin1')  # dictionary type, we use 'latin1' for python3\n",
    "\n",
    "        # Don't forget to change names for mean and std files when preprocessing for grayscale purposes\n",
    "        d['x_train'] /= std['std_' + colour]\n",
    "        d['x_validation'] /= std['std_' + colour]\n",
    "        d['x_test'] /= std['std_' + colour]\n",
    "\n",
    "    # WARNING!\n",
    "    # Do not make transpose starting from data1\n",
    "    # As data0 was already transposed\n",
    "    if transpose:\n",
    "        # Transposing every dataset to make channels come first\n",
    "        d['x_train'] = d['x_train'].transpose(0, 3, 1, 2)  # (86989, 3, 32, 32)\n",
    "        d['x_validation'] = d['x_validation'].transpose(0, 3, 1, 2)  # (86989, 3, 32, 32)\n",
    "        d['x_test'] = d['x_test'].transpose(0, 3, 1, 2)  # (86989, 3, 32, 32)\n",
    "\n",
    "    # Returning preprocessed data\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 43/43 [00:42<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Backward Calculation\n",
      "[ 0 11 11  9  8  9  7  5  5  8 11 11 10 10 11 12 13 15 16 15 12 10  9  8\n",
      "  7  5  4  3  2  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "## Option 1 - rgb data --> starts here\n",
    "\n",
    "# Loading rgb data from training dataset\n",
    "x_train, y_train, s_train, c_train = load_rgb_data('data\\\\train.pickle')\n",
    "#\n",
    "# # Loading rgb data from validation dataset\n",
    "x_validation, y_validation, s_validation, c_validation = load_rgb_data('data\\\\valid.pickle')\n",
    "#\n",
    "# # Loading rgb data from test dataset\n",
    "x_test, y_test, s_test, c_test = load_rgb_data('data\\\\test.pickle')\n",
    "#\n",
    "# # Getting texts for every class\n",
    "label_list = label_text('data\\\\label_names.csv')\n",
    "#\n",
    "# # Plotting 43 unique examples with their label's names\n",
    "# # And histogram of 43 classes with their number of examples\n",
    "#plot_unique_examples(x_train, y_train)\n",
    "#\n",
    "# # Plotting 43 good quality examples to show in GUI for driver\n",
    "#plot_signs()\n",
    "#\n",
    "# # Implementing equalization of training dataset\n",
    "x_train, y_train = equalize_training_dataset(x_train.astype(np.uint8), y_train)\n",
    "#\n",
    "# # Plotting 43 unique examples with their label's names\n",
    "# # And histogram of 43 classes with their number of examples\n",
    "# plot_unique_examples(x_train, y_train)\n",
    "#\n",
    "# # Putting loaded and equalized data into the dictionary\n",
    "# # Equalization is done only for training dataset\n",
    "d_loaded = {'x_train': x_train, 'y_train': y_train,\n",
    "            'x_validation': x_validation, 'y_validation': y_validation,\n",
    "            'x_test': x_test, 'y_test': y_test,\n",
    "            'labels': label_list}\n",
    "\n",
    "\n",
    "# WARNING! It is important to run different preprocessing approaches separately\n",
    "# Otherwise, dictionary will change values increasingly\n",
    "# Also, creating separate dictionaries like 'd0, d1, d2, d3' will not help\n",
    "# As they all contain same references to the datasets\n",
    "\n",
    "\n",
    "# # Applying preprocessing\n",
    "data0 = preprocess_data(d_loaded, shuffle=True, transpose=True)\n",
    "print('Before Backward Calculation')\n",
    "print(data0['x_train'][0, 0, :, 0])\n",
    "# Saving loaded and preprocessed data into 'pickle' file\n",
    "with open('data0.pickle', 'wb') as f:\n",
    "    pickle.dump(data0, f)\n",
    "    # Releasing memory\n",
    "del data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0,   0,   0, ...,  24,  20,   0],\n",
       "         [ 11,  15,  16, ...,  24,  21,   0],\n",
       "         [ 11,  12,  10, ...,  20,  22,   0],\n",
       "         ...,\n",
       "         [  0,  18,  16, ...,  18,  17,  18],\n",
       "         [  0,  17,  15, ...,  19,  18,  17],\n",
       "         [  0,  17,  17, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,  28,  22,   0],\n",
       "         [ 13,  17,  19, ...,  28,  24,   0],\n",
       "         [ 13,  14,  12, ...,  24,  24,   0],\n",
       "         ...,\n",
       "         [  0,  19,  15, ...,  17,  18,  19],\n",
       "         [  0,  19,  16, ...,  17,  17,  16],\n",
       "         [  0,  18,  18, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,  20,  16,   0],\n",
       "         [ 15,  21,  24, ...,  21,  18,   0],\n",
       "         [ 15,  16,  14, ...,  18,  18,   0],\n",
       "         ...,\n",
       "         [  0,  20,  15, ...,  13,  14,  15],\n",
       "         [  0,  19,  14, ...,  14,  12,  12],\n",
       "         [  0,  17,  15, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[  0,   0,   0, ...,  16,   8,   0],\n",
       "         [  9,  11,  13, ...,  16,   9,   0],\n",
       "         [ 18,  18,  18, ...,  15,  10,   0],\n",
       "         ...,\n",
       "         [  0,  14,  15, ...,  21,  22,  21],\n",
       "         [  0,  12,  15, ...,  18,  17,  14],\n",
       "         [  0,  10,  15, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,  15,   7,   0],\n",
       "         [  9,  10,  11, ...,  14,   9,   0],\n",
       "         [ 16,  16,  15, ...,  13,   9,   0],\n",
       "         ...,\n",
       "         [  0,  11,  12, ...,  20,  21,  20],\n",
       "         [  0,   9,  11, ...,  16,  15,  13],\n",
       "         [  0,   7,  12, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,  14,   7,   0],\n",
       "         [  9,  10,  11, ...,  14,   9,   0],\n",
       "         [ 16,  16,  15, ...,  13,   9,   0],\n",
       "         ...,\n",
       "         [  0,  11,  12, ...,  20,  22,  19],\n",
       "         [  0,   9,  11, ...,  16,  15,  12],\n",
       "         [  0,   7,  12, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[ 25,  25,  26, ...,  68,  44,  29],\n",
       "         [ 26,  26,  25, ...,  66,  44,  30],\n",
       "         [ 26,  25,  24, ...,  66,  44,  30],\n",
       "         ...,\n",
       "         [255, 250, 208, ...,  74,  69,  70],\n",
       "         [255, 249, 198, ...,  74,  78,  61],\n",
       "         [253, 249, 200, ...,  75,  83,  64]],\n",
       "\n",
       "        [[ 28,  27,  27, ...,  70,  52,  31],\n",
       "         [ 28,  28,  27, ...,  69,  51,  31],\n",
       "         [ 28,  28,  28, ...,  68,  52,  31],\n",
       "         ...,\n",
       "         [255, 255, 231, ...,  79,  78,  65],\n",
       "         [254, 255, 231, ...,  78,  84,  66],\n",
       "         [238, 255, 229, ...,  79,  87,  70]],\n",
       "\n",
       "        [[ 34,  33,  33, ...,  79,  62,  34],\n",
       "         [ 35,  34,  33, ...,  77,  62,  35],\n",
       "         [ 34,  34,  33, ...,  76,  62,  34],\n",
       "         ...,\n",
       "         [255, 255, 246, ...,  84,  84,  65],\n",
       "         [250, 255, 249, ...,  82,  94,  75],\n",
       "         [226, 254, 247, ...,  85,  96,  78]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[  0,   0,   0, ...,  21,  15,   0],\n",
       "         [  8,  10,  11, ...,  18,  15,   0],\n",
       "         [ 12,  11,  10, ...,  20,  15,   0],\n",
       "         ...,\n",
       "         [  0,  11,  13, ...,  22,  19,  14],\n",
       "         [  0,  10,  12, ...,  20,  19,  12],\n",
       "         [  0,   9,  10, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,  23,  16,   0],\n",
       "         [  7,   9,  10, ...,  22,  19,   0],\n",
       "         [ 11,  11,  10, ...,  22,  19,   0],\n",
       "         ...,\n",
       "         [  0,  10,  11, ...,  21,  16,  11],\n",
       "         [  0,   9,  11, ...,  19,  16,  10],\n",
       "         [  0,   8,  10, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,  28,  18,   0],\n",
       "         [  7,   9,  10, ...,  29,  23,   0],\n",
       "         [ 11,  12,  12, ...,  30,  25,   0],\n",
       "         ...,\n",
       "         [  0,  10,  13, ...,  23,  17,  11],\n",
       "         [  0,   9,  12, ...,  21,  18,  11],\n",
       "         [  0,   9,  11, ...,   0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[  0,  22,  25, ...,   4,   1,   0],\n",
       "         [  0,  25,  26, ...,  47,  50,  43],\n",
       "         [  1,  26,  24, ...,  42,  46,  41],\n",
       "         ...,\n",
       "         [ 62,  66,  69, ...,  88,  79,   7],\n",
       "         [ 48,  58,  65, ...,  94,  90,   3],\n",
       "         [  0,   0,   2, ...,  95,  90,   0]],\n",
       "\n",
       "        [[  0,  19,  23, ...,   4,   1,   0],\n",
       "         [  0,  21,  23, ...,  58,  57,  50],\n",
       "         [  0,  22,  20, ...,  48,  51,  47],\n",
       "         ...,\n",
       "         [ 61,  61,  64, ...,  83,  75,   6],\n",
       "         [ 46,  54,  61, ...,  93,  93,   3],\n",
       "         [  0,   0,   2, ...,  97,  95,   0]],\n",
       "\n",
       "        [[  0,  17,  21, ...,   5,   2,   0],\n",
       "         [  0,  18,  20, ...,  61,  64,  58],\n",
       "         [  0,  18,  17, ...,  53,  58,  55],\n",
       "         ...,\n",
       "         [ 57,  57,  60, ...,  76,  65,   6],\n",
       "         [ 43,  49,  55, ...,  86,  82,   2],\n",
       "         [  0,   0,   1, ...,  90,  84,   0]]],\n",
       "\n",
       "\n",
       "       [[[  0,   0,   0, ...,   3,   0,   0],\n",
       "         [  0,   0,   0, ...,   6,   0,   0],\n",
       "         [  2,   4,   6, ...,  10,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,  10, ...,   8,   5,   3],\n",
       "         [  0,   0,   8, ...,   0,   0,   0],\n",
       "         [  0,   0,   6, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,   2,   0,   0],\n",
       "         [  0,   0,   0, ...,   5,   0,   0],\n",
       "         [  2,   5,   7, ...,   9,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,  11, ...,   8,   5,   4],\n",
       "         [  0,   0,   9, ...,   0,   0,   0],\n",
       "         [  0,   0,   6, ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0, ...,   3,   0,   0],\n",
       "         [  0,   0,   0, ...,   6,   0,   0],\n",
       "         [  3,   6,   9, ...,   9,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,  11, ...,  11,   8,   5],\n",
       "         [  0,   0,   8, ...,   0,   0,   0],\n",
       "         [  0,   0,   6, ...,   0,   0,   0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_loaded['x_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-6180afe84fd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loaded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'example.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36mwrite_table\u001b[1;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, **kwargs)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         with ParquetWriter(\n\u001b[1;32m-> 1331\u001b[1;33m                 \u001b[0mwhere\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m                 \u001b[0mfilesystem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilesystem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m                 \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'schema'"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "pq.write_table(d_loaded['x_train'], 'example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
