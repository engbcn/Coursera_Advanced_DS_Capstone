{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Feature Engineering Notebook", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Download additional python packages", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting opencv-python\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/7e/bd5425f4dacb73367fddc71388a47c1ea570839197c2bcad86478e565186/opencv_python-4.1.1.26-cp36-cp36m-manylinux1_x86_64.whl (28.7MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.7MB 10.4MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from opencv-python) (1.15.4)\nInstalling collected packages: opencv-python\nSuccessfully installed opencv-python-4.1.1.26\n"
                }
            ], 
            "source": "!pip install opencv-python"
        }, 
        {
            "source": "Install libraries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Importing standard libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\n\nfrom tqdm import tqdm\nimport cv2\nfrom sklearn.utils import shuffle\nfrom botocore.client import Config\nimport ibm_boto3"
        }, 
        {
            "source": "Define credentials to access a IBM cloud bucket storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "Download the raw data into a dictionary", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cos.download_file(Bucket=credentials['BUCKET'], Key='raw_data.pickle', Filename='raw_data.pickle')   \n\nwith open('raw_data.pickle', 'rb') as f:\n    raw_data = pickle.load(f, encoding='latin1')  # dictionary type"
        }, 
        {
            "source": "We use a set of functions for augmentating the amount of data in the training set. This functions have been adapted from https://www.kaggle.com/valentynsichkar/traffic-signs-preprocessed#datasets_preparing.py", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Defining function for equalization training dataset\ndef equalize_training_dataset(x_train, y_train):\n    # Getting number of examples for every label\n    number_of_examples_for_every_label = np.bincount(y_train)\n    # Calculating total amount of unique labels\n    number_of_labels = np.arange(len(number_of_examples_for_every_label))\n\n    # Iterating over all number of labels\n    # Showing progress ber with 'tqdm'\n    for i in tqdm(number_of_labels):\n        # Calculating how many examples is needed to add for current label\n        # 'np.mean(number_of_examples_for_every_label)' - average number over examples for every label\n        number_of_examples_to_add = int(np.mean(number_of_examples_for_every_label) * 2.5) - \\\n                                    number_of_examples_for_every_label[i]\n\n        # Defining temporary arrays for collecting new images\n        x_temp = []\n        y_temp = []\n\n        # Getting random image from current label\n        # Transforming it and adding to the temporary arrays\n        for j in range(number_of_examples_to_add):\n            getting_random_image = random_image(x_train, y_train, i)\n            x_temp.append(transformation_brightness_rotation(getting_random_image))\n            y_temp.append(i)\n\n        x_train = np.append(x_train, np.array(x_temp), axis=0)\n        y_train = np.append(y_train, np.array(y_temp), axis=0)\n\n    return x_train, y_train\n\n# Defining function for getting random image of one label\ndef random_image(x_train, y_train, y_number):\n    # Getting indexes of needed 'y_number' from 'y_train'\n    # Defining True - False array\n    image_indexes = np.where(y_train == y_number)\n    # Getting random index of needed label\n    # 'np.bincount(y_train)' - array with number of examples for every label\n    # 'np.bincount(y_train)[y_number] - 1' - number of examples for 'y_number' label\n    random_index = np.random.randint(0, np.bincount(y_train)[y_number] - 1)\n    # Returning random image from 'x_train'\n    # 'x_train[image_indexes]' - returns array with only 'y_number' label\n    # 'x_train[image_indexes][random_index]' - random image of needed label\n    return x_train[image_indexes][random_index]\n\n# Defining function for transformation: brightness + rotation\ndef transformation_brightness_rotation(image):\n    return brightness_changing(rotation_changing(image))\n\n# Defining function for changing brightness\ndef brightness_changing(image):\n    # Converting firstly image from RGB to HSV\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n    # Defining random value for changing brightness\n    random_brightness = 0.25 + np.random.uniform()\n    # Implementing changing of Value channel of HSV image\n    image_hsv[:, :, 2] = image_hsv[:, :, 2] * random_brightness\n    # Converting HSV changed image to RGB\n    image_rgb = cv2.cvtColor(image_hsv, cv2.COLOR_HSV2RGB)\n    # Returning image with changed brightness\n    return image_rgb\n\n# Defining function for changing rotation of image\ndef rotation_changing(image):\n    # Defining angle range\n    angle_range = 25\n    # Defining angle rotation\n    angle_rotation = np.random.uniform(angle_range) - angle_range / 2\n    # Getting shape of image\n    rows, columns, channels = image.shape\n    # Implementing rotation\n    # Calculating Affine Matrix\n    affine_matrix = cv2.getRotationMatrix2D((columns / 2, rows / 2), angle_rotation, 1)\n    # Warping original image with Affine Matrix\n    rotated_image = cv2.warpAffine(image, affine_matrix, (columns, rows))\n    # Returning rotated image\n    return rotated_image\n"
        }, 
        {
            "source": "We perform data augmentation on the training data set", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 43/43 [01:04<00:00,  1.78s/it]\n"
                }
            ], 
            "source": "# # Implementing equalization of training dataset\nx_train, y_train = equalize_training_dataset(raw_data['x_train'].astype(np.uint8), raw_data['y_train'])\n\n# # Putting loaded and equalized data into the dictionary\n# # Equalization is done only for training dataset\nd_loaded = {'x_train': x_train, 'y_train': y_train,\n            'x_validation': raw_data['x_validation'], 'y_validation': raw_data['y_validation'],\n            'x_test': raw_data['x_test'], 'y_test': raw_data['y_test'],\n            'labels': raw_data['labels']}"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open('data_equalized.pickle', 'wb') as f:\n    pickle.dump(d_loaded, f)\n    # Releasing memory\n    del d_loaded\n    \n#upload feather object\ncos.upload_file(Filename='data_equalized.pickle', Bucket=credentials['BUCKET'], Key='data_equalized.pickle') "
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cos.download_file(Bucket=credentials['BUCKET'], Key='data_equalized.pickle', Filename='data_equalized.pickle')   \n\nwith open('data_equalized.pickle', 'rb') as f:\n    d_loaded = pickle.load(f, encoding='latin1')  # dictionary type"
        }, 
        {
            "source": "We shuffle training data. We also do it with validation and test data although it is not necessary", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "d_loaded['x_train'], d_loaded['y_train'] = shuffle(d_loaded['x_train'], d_loaded['y_train'], random_state = 0)\nd_loaded['x_validation'], d_loaded['y_validation'] = shuffle(d_loaded['x_validation'], d_loaded['y_validation'], random_state = 0)\nd_loaded['x_test'], d_loaded['y_test'] = shuffle(d_loaded['x_test'], d_loaded['y_test'], random_state = 0)"
        }, 
        {
            "source": "We transpose data images in order color channels are first", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(86989, 32, 32, 3)\n(4410, 32, 32, 3)\n(12630, 32, 32, 3)\n"
                }
            ], 
            "source": "print(d_loaded['x_train'].shape)\nprint(d_loaded['x_validation'].shape)\nprint(d_loaded['x_test'].shape)"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "d_loaded['x_train'] = d_loaded['x_train'].transpose(0, 3, 1, 2)\nd_loaded['x_validation'] = d_loaded['x_validation'].transpose(0, 3, 1, 2)\nd_loaded['x_test'] = d_loaded['x_test'].transpose(0, 3, 1, 2)"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(86989, 3, 32, 32)\n(4410, 3, 32, 32)\n(12630, 3, 32, 32)\n"
                }
            ], 
            "source": "print(d_loaded['x_train'].shape)\nprint(d_loaded['x_validation'].shape)\nprint(d_loaded['x_test'].shape)"
        }, 
        {
            "source": "As input data is only unstructured data with pixel value volors, we can normalize the data dividing by 255 instead of calculating mean and standard deviation.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open('data_shuffled.pickle', 'wb') as f:\n    pickle.dump(d_loaded, f)\n    # Releasing memory\n    del d_loaded\n    \n#upload feather object\ncos.upload_file(Filename='data_shuffled.pickle', Bucket=credentials['BUCKET'], Key='data_shuffled.pickle') "
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cos.download_file(Bucket=credentials['BUCKET'], Key='data_shuffled.pickle', Filename='data_shuffled.pickle')   \n\nwith open('data_shuffled.pickle', 'rb') as f:\n    d_loaded = pickle.load(f, encoding='latin1')  # dictionary type"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Normalizing whole data by dividing /255.0\nd_loaded['x_train'] = d_loaded['x_train'].astype(np.float32) / 255.0\nd_loaded['x_validation'] = d_loaded['x_validation'].astype(np.float32) / 255.0\nd_loaded['x_test'] = d_loaded['x_test'].astype(np.float32) / 255.0"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open('data_shuffled_scaled255.pickle', 'wb') as f:\n    pickle.dump(d_loaded, f)\n    # Releasing memory\n    del d_loaded\n    \n#upload feather object\ncos.upload_file(Filename='data_shuffled_scaled255.pickle', Bucket=credentials['BUCKET'], Key='data_shuffled_scaled255.pickle') "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}